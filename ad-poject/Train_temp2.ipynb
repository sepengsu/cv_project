{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from trainer import EarlyStopping,model_delete,mytrainsform, AugmentedDataset, dir_clear\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세팅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Computational device\n",
    "# Device will be set to GPU if it is available.(you should install valid Pytorch version with CUDA. Otherwise, it will be computed using CPU)\n",
    "from model import MODEL_CLASS2\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "DIR = './results/temp_2'\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)\n",
    "print(\"Using Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train = True,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())\n",
    "testset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train     = False,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6000 Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "SELECT_NORMAL = 2 # Set 2 class as train dataset.\n",
    "trainset.data = trainset.data[trainset.targets == SELECT_NORMAL]\n",
    "trainset.targets = trainset.targets[trainset.targets == SELECT_NORMAL] # Set 2 class as train dataset.\n",
    "\n",
    "test_label = [2,4,6] # Define actual test class that we use\n",
    "actual_testdata = torch.isin(testset.targets, torch.tensor(test_label))\n",
    "testset.data = testset.data[actual_testdata]\n",
    "testset.targets = testset.targets[actual_testdata]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = testset, batch_size  = 1,\n",
    "    shuffle     = False,num_workers = 2)\n",
    "\n",
    "train_data_size = len(trainset)\n",
    "test_data_size = len(testset)\n",
    "\n",
    "print(\"Train data size:\", train_data_size, \"Test data size:\", test_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 증강 기법 사용 class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 24000 Val data size: 1200 Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 먼저 train과 val로 나누고, train에 대해서만 증강을 적용\n",
    "n_val = int(len(trainset) * 0.2)\n",
    "n_train = len(trainset) - n_val\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "augset, valset = torch.utils.data.random_split(trainset, [n_train, n_val], generator=torch.Generator().manual_seed(2025))\n",
    "\n",
    "augset = AugmentedDataset(augset, transform=mytrainsform, augmentation_factor=5) # augmentation_factor = 5 for baseline 모델 찾기 \n",
    "# valset은 증강을 적용하지 않음\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = augset, batch_size  = BATCH_SIZE,\n",
    "    shuffle     = True,num_workers = 0) \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = valset, batch_size = BATCH_SIZE,\n",
    "    shuffle     = False,num_workers = 0)\n",
    "\n",
    "# data size check\n",
    "print(\"Train data size:\", len(augset),\"Val data size:\", len(valset),\"Test data size:\", len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 및 loss 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes  = MODEL_CLASS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available reconstruction loss functions: dict_keys(['MSE'])\n",
      "Available diffusion loss functions: dict_keys(['MSE', 'MSE+Gradient'])\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "# loss function 추천 조합\n",
    "from loss.losses import FlexibleLoss, FlexibleDiffusionLoss\n",
    "\n",
    "reconstruction_loss = {\"MSE\": FlexibleLoss(mode=\"mse\"),}\n",
    "\n",
    "diffusion_loss = {\n",
    "    \"MSE\": FlexibleDiffusionLoss(mode=\"mse\"),\n",
    "    \"MSE+Gradient\": FlexibleDiffusionLoss(mode=\"mse+gradient\", beta=1.0, alpha=0.1),\n",
    "}\n",
    "loss_functions = {\n",
    "    \"reconstruction\": reconstruction_loss,\n",
    "    \"diffusion\": diffusion_loss,\n",
    "}\n",
    "\n",
    "print(\"Available reconstruction loss functions:\", reconstruction_loss.keys())\n",
    "print(\"Available diffusion loss functions:\", diffusion_loss.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 0 models and loss functions will be deleted: []\n",
      "✅ 2 models and loss functions will be kept: ['SlimDeepCAE_Bottleneck32_Dropout', 'SlimDeepCAE_Combo']\n",
      "Remaining models: dict_keys(['SlimDeepCAE_Bottleneck32_Dropout', 'SlimDeepCAE_Combo'])\n"
     ]
    }
   ],
   "source": [
    "# 모델과 loss function 조합을 삭제\n",
    "model_classes = model_delete(model_classes, loss_functions, DIR)\n",
    "print(\"Remaining models:\", model_classes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch image shape: torch.Size([1024, 1, 28, 28])\n",
      "Train batch label shape: torch.Size([1024])\n",
      "Test batch image shape: torch.Size([1, 1, 28, 28])\n",
      "Test batch label shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of a batch from train_loader and test_loader\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "print(\"Train batch image shape:\", train_images.shape)\n",
    "print(\"Train batch label shape:\", train_labels.shape)\n",
    "print(\"Test batch image shape:\", test_images.shape)\n",
    "print(\"Test batch label shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ./results/temp_2/logs cleared.\n",
      "✅ ./results/temp_2/checkpoints cleared.\n",
      "✅ ./results/temp_2/eval_results cleared.\n"
     ]
    }
   ],
   "source": [
    "dir_clear(DIR + \"/logs\")\n",
    "dir_clear(DIR + \"/checkpoints\")\n",
    "dir_clear(DIR + \"/eval_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Total Models: 2\n",
      "Reconstruction Losses: 1 Diffusion Losses: 2\n",
      "Total Combinations: 2\n",
      "==================================================\n",
      "▶ Training [SlimDeepCAE_Bottleneck32_Dropout] with [MSE] (FP16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SlimDeepCAE_Bottleneck32_Dropout | MSE (FP16):   0%|          | 0/500 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchTrainerFP16' object has no attribute '_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GridSearchTrainerFP16\n\u001b[32m      5\u001b[39m trainer = GridSearchTrainerFP16(\n\u001b[32m      6\u001b[39m     models=model_classes,\n\u001b[32m      7\u001b[39m     criterions_dict=loss_functions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     log_dir=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/logs\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m results_df = pd.DataFrame(results)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Save the results to a CSV file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\na062\\Desktop\\cv_project\\ad-poject\\trainer\\gridsearch.py:79\u001b[39m, in \u001b[36mGridSearchTrainerFP16.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m.n_epochs, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (FP16)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     78\u001b[39m     train_loss = \u001b[38;5;28mself\u001b[39m.train_one_epoch(model, optimizer, criterion_in_trainer)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     val_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_in_trainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     scheduler.step()\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\na062\\Desktop\\cv_project\\ad-poject\\trainer\\trainer.py:45\u001b[39m, in \u001b[36mOneEpochTrainerFP16.validate_one_epoch\u001b[39m\u001b[34m(self, model, criterion)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m     44\u001b[39m     x = x.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m(model, x, criterion, is_train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m     total_loss += loss.item()\n\u001b[32m     47\u001b[39m     pbar.set_postfix(loss=loss.item())\n",
      "\u001b[31mAttributeError\u001b[39m: 'GridSearchTrainerFP16' object has no attribute '_step'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500\n",
    "PATIENCE = 40\n",
    "# GridSearchTrainerfp16\n",
    "from trainer import GridSearchTrainerFP16\n",
    "trainer = GridSearchTrainerFP16(\n",
    "    models=model_classes,\n",
    "    criterions_dict=loss_functions,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    save_dir=f'{DIR}/checkpoints',\n",
    "    verbose=False, \n",
    "    device=DEVICE,\n",
    "    lr=1e-3 * BATCH_SIZE / 256, # default learning rate for AdamW\n",
    "    log_dir=f'{DIR}/logs',\n",
    ")\n",
    "results = trainer.run()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(f'{DIR}/training.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>loss</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>gpu_peak_usage(MB)</th>\n",
       "      <th>save_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SlimDeepCAE_Bottleneck16_Dropout</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.021140</td>\n",
       "      <td>41</td>\n",
       "      <td>70.255371</td>\n",
       "      <td>./results/temp_2/checkpoints/SlimDeepCAE_Bottl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SlimDeepCAE_Bottleneck8_Dropout</td>\n",
       "      <td>MSE</td>\n",
       "      <td>0.042504</td>\n",
       "      <td>19</td>\n",
       "      <td>63.175781</td>\n",
       "      <td>./results/temp_2/checkpoints/SlimDeepCAE_Bottl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model loss  best_val_loss  best_epoch  \\\n",
       "0  SlimDeepCAE_Bottleneck16_Dropout  MSE       0.021140          41   \n",
       "1   SlimDeepCAE_Bottleneck8_Dropout  MSE       0.042504          19   \n",
       "\n",
       "   gpu_peak_usage(MB)                                          save_path  \n",
       "0           70.255371  ./results/temp_2/checkpoints/SlimDeepCAE_Bottl...  \n",
       "1           63.175781  ./results/temp_2/checkpoints/SlimDeepCAE_Bottl...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir ./results/temp_2/logs\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensorboard --logdir {DIR}/logs\") # tensorboard --logdir ./results/gridsearch16/logs/1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
