{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab8b0cc-dc91-4635-8cb5-52e58662c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af804943",
   "metadata": {},
   "source": [
    "### 세팅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d106579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Computational device\n",
    "# Device will be set to GPU if it is available.(you should install valid Pytorch version with CUDA. Otherwise, it will be computed using CPU)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6cf9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train = True,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())\n",
    "testset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train     = False,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0501b7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6000\n",
      "Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "SELECT_NORMAL = 2 # Set 2 class as train dataset.\n",
    "trainset.data = trainset.data[trainset.targets == SELECT_NORMAL]\n",
    "trainset.targets = trainset.targets[trainset.targets == SELECT_NORMAL] # Set 2 class as train dataset.\n",
    "\n",
    "test_label = [2,4,6] # Define actual test class that we use\n",
    "actual_testdata = torch.isin(testset.targets, torch.tensor(test_label))\n",
    "testset.data = testset.data[actual_testdata]\n",
    "testset.targets = testset.targets[actual_testdata]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = testset, batch_size  = 1,\n",
    "    shuffle     = False,num_workers = 2)\n",
    "\n",
    "train_data_size = len(trainset)\n",
    "test_data_size = len(testset)\n",
    "\n",
    "print(\"Train data size:\", train_data_size)\n",
    "print(\"Test data size:\", test_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee387102",
   "metadata": {},
   "source": [
    "#### 데이터 증강 기법 사용 class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81204f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.1):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = x.data.new(x.size()).normal_(0, self.std)\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d8fd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 배로 Augmentation을 할 것인지 알려주면 해당 배수만큼 Augmentation을 수행하는 클래스\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    GaussianNoise(0.1)\n",
    "])\n",
    "\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None, augmentation_factor=1):\n",
    "        '''\n",
    "        dataset: 원본 데이터셋\\\n",
    "        transform: 증강을 위한 transform\n",
    "        augmentation_factor: 몇 배로 Augmentation\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.augmentation_factor = augmentation_factor\n",
    "        self.original_length = len(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 전체 데이터 수 = 원본 * 배수\n",
    "        return self.original_length * self.augmentation_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 인덱스를 순환해서 접근\n",
    "        original_idx = idx % self.original_length\n",
    "        x, y = self.dataset[original_idx]\n",
    "\n",
    "        # 증강 적용\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c358e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 96000 Val data size: 4800 Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 먼저 train과 val로 나누고, train에 대해서만 증강을 적용\n",
    "n_val = int(len(trainset) * 0.2)\n",
    "n_train = len(trainset) - n_val\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [n_train, n_val], generator=torch.Generator().manual_seed(2025))\n",
    "\n",
    "trainset = AugmentedDataset(trainset, transform=transform, augmentation_factor=5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = trainset, batch_size  = BATCH_SIZE,\n",
    "    shuffle     = True,num_workers = 2)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = valset, batch_size  = BATCH_SIZE,\n",
    "    shuffle     = False,num_workers = 2)\n",
    "\n",
    "# data size check\n",
    "print(\"Train data size:\", len(trainset),\"Val data size:\", len(valset),\"Test data size:\", len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d187298",
   "metadata": {},
   "source": [
    "### 모델 및 Training Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7be92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EaryStopping():\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        '''\n",
    "        patience (int): 얼마나 기다릴지\n",
    "        verbose (bool): True일 경우 각 epoch의 loss 출력\n",
    "        delta (float): 개선이 되었다고 인정되는 최소한의 loss\n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss # validation loss가 작을수록 좋다고 가정\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e343ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSelector():\n",
    "    def __init__(self, model, is_2d=True):\n",
    "        self.model = model\n",
    "        self.is_2d = is_2d\n",
    "    \n",
    "    def __call__(self, train_loader, optimizer, criterion):\n",
    "        if self.is_2d:\n",
    "            return self._train2d(self.model, train_loader,optimizer,criterion)\n",
    "        else:\n",
    "            return self._train1d(self.model, train_loader,optimizer,criterion)\n",
    "    def _train2d(model, train_loader, optimizer, criterion):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        return train_loss / len(train_loader) # Average loss per batch\n",
    "    \n",
    "    def _train1d(model,train_loader,optimizer,criterion):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(DEVICE).view(-1, 28*28) # Flatten 2D data to 1D\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        return train_loss / len(train_loader) # Average loss per batch\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36d91f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.autoencoder import Autoencoder2D, Autoencoder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "model = Autoencoder2D().to(DEVICE) # Autoencoder2D is used for 2D data.\n",
    "Criterion = nn.MSELoss() # Mean Squared Error is used as loss function.\n",
    "Optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE) # AdamW optimizer is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d50c72",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c26c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SummaryWriter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training process\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m writer = \u001b[43mSummaryWriter\u001b[49m()\n\u001b[32m      3\u001b[39m early_stopping = EaryStopping(patience=\u001b[32m10\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m train_selector = TrainSelector(model, is_2d=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SummaryWriter' is not defined"
     ]
    }
   ],
   "source": [
    "# Training process\n",
    "writer = SummaryWriter()\n",
    "early_stopping = EaryStopping(patience=10, verbose=True)\n",
    "train_selector = TrainSelector(model, is_2d=True)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss = train_selector(train_loader, Optimizer, Criterion)\n",
    "    val_loss = train_selector(val_loader, Optimizer, Criterion)\n",
    "    writer.add_scalars('Loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42dce3",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "THRESHOLDVAL=0.01 # threshold val\n",
    "dic_loss = {'id':[], 'label':[], 'score':[],'normal':[]}\n",
    "\n",
    "count=0\n",
    "for step, (x, label) in enumerate(test_loader):\n",
    "    x = x.view(-1, 28*28).to(DEVICE)\n",
    "    y = x.view(-1, 28*28).to(DEVICE) \n",
    "\n",
    "    encoded, decoded = model(x)\n",
    "    loss = float(criterion(decoded, y).cpu().detach().numpy())\n",
    "    dic_loss['id'].append(step)\n",
    "    dic_loss['label'].append(int(label==SELECT_NORMAL)) # 1: normal, 0: abnormal\n",
    "    dic_loss['score'].append(loss) # abnormal score\n",
    "    if loss>THRESHOLDVAL: dic_loss['normal'].append('0')\n",
    "    else: dic_loss['normal'].append('1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c634e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gernerating a plot\n",
    "arr_label = np.array(dic_loss['label'])\n",
    "arr_score = np.array(dic_loss['score'])\n",
    "score_min = arr_score.min()\n",
    "score_max = arr_score.max()\n",
    "plt.hist(arr_score[np.where(arr_label == 1)[0]], bins=30, range=(score_min, score_max), alpha=0.5, label='Normal')\n",
    "plt.hist(arr_score[np.where(arr_label == 0)[0]], bins=30, range=(score_min, score_max), alpha=0.5, label='Abnormal')\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(THRESHOLDVAL,0,1, color='red',linestyle='--',linewidth=1)\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(\"plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating AUROC\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n",
    "fpr, tpr, thresholds = roc_curve(dic_loss['label'], dic_loss['score'], pos_label=0)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.savefig(\"auroc.png\")\n",
    "plt.show()\n",
    "auroc = auc(fpr, tpr)\n",
    "print(\"AUROC: {}\".format(auroc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e971b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leveraging the pandas library to convert a dict to a dataframe is more convenient when checking values.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\n",
    "df = pd.DataFrame.from_dict(dic_loss)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a46acf-7944-4bf9-9a04-c911c3973fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to submit .csv file to kaggle, dataframe should fit following format\n",
    "# id[1,2,3...,3000], predicted anomalies[0,1,0....,0]\n",
    "\n",
    "# 'pop('score,None')' delete one of the item in dict\n",
    "# 'del df['item']', is also available.\n",
    "\n",
    "# If you try to remove invalid itmes in the dict, message that you set will be returned.\n",
    "# set to None, nothing will be returned\n",
    "dic_loss.pop('score',None)\n",
    "dic_loss.pop('label',None)\n",
    "df = pd.DataFrame.from_dict(dic_loss)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12310bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_csv command will convert your dict to .csv file with the name of your teamnumber\n",
    "# Do not forget to submit the .csv file to kaggle. If you upload .csv file properly to kaggle, you can check your result immediately at the leaderboard.\n",
    "teamnumber = 8 # insert your teamnumber\n",
    "df.to_csv(\"result_team{}.csv\".format(teamnumber), index =False) # Index should be not included in the .csv file.\n",
    "torch.save(model.state_dict(), 'model_team{}.pth'.format(teamnumber))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb22687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
