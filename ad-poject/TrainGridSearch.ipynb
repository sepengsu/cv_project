{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 세팅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Computational device\n",
    "# Device will be set to GPU if it is available.(you should install valid Pytorch version with CUDA. Otherwise, it will be computed using CPU)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"Using Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST dataset\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train = True,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())\n",
    "testset = datasets.FashionMNIST(\n",
    "    root      = './.data/', train     = False,\n",
    "    download  = True,\n",
    "    transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6000 Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "SELECT_NORMAL = 2 # Set 2 class as train dataset.\n",
    "trainset.data = trainset.data[trainset.targets == SELECT_NORMAL]\n",
    "trainset.targets = trainset.targets[trainset.targets == SELECT_NORMAL] # Set 2 class as train dataset.\n",
    "\n",
    "test_label = [2,4,6] # Define actual test class that we use\n",
    "actual_testdata = torch.isin(testset.targets, torch.tensor(test_label))\n",
    "testset.data = testset.data[actual_testdata]\n",
    "testset.targets = testset.targets[actual_testdata]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = testset, batch_size  = 1,\n",
    "    shuffle     = False,num_workers = 2)\n",
    "\n",
    "train_data_size = len(trainset)\n",
    "test_data_size = len(testset)\n",
    "\n",
    "print(\"Train data size:\", train_data_size, \"Test data size:\", test_data_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 증강 기법 사용 class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.1):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = x.data.new(x.size()).normal_(0, self.std)\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몇 배로 Augmentation을 할 것인지 알려주면 해당 배수만큼 Augmentation을 수행하는 클래스\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    GaussianNoise(0.1)\n",
    "])\n",
    "\n",
    "class AugmentedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None, augmentation_factor=1):\n",
    "        '''\n",
    "        dataset: 원본 데이터셋\\\n",
    "        transform: 증강을 위한 transform\n",
    "        augmentation_factor: 몇 배로 Augmentation\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.augmentation_factor = augmentation_factor\n",
    "        self.original_length = len(dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 전체 데이터 수 = 원본 * 배수\n",
    "        return self.original_length * self.augmentation_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 원본 인덱스를 순환해서 접근\n",
    "        original_idx = idx % self.original_length\n",
    "        x, y = self.dataset[original_idx]\n",
    "\n",
    "        # 증강 적용\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 48000 Val data size: 1200 Test data size: 3000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 먼저 train과 val로 나누고, train에 대해서만 증강을 적용\n",
    "n_val = int(len(trainset) * 0.2)\n",
    "n_train = len(trainset) - n_val\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "augset, valset = torch.utils.data.random_split(trainset, [n_train, n_val], generator=torch.Generator().manual_seed(2025))\n",
    "\n",
    "augset = AugmentedDataset(augset, transform=transform, augmentation_factor=10)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = augset, batch_size  = BATCH_SIZE,\n",
    "    shuffle     = True,num_workers = 0) \n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = valset, batch_size = BATCH_SIZE,\n",
    "    shuffle     = False,num_workers = 0)\n",
    "\n",
    "# data size check\n",
    "print(\"Train data size:\", len(augset),\"Val data size:\", len(valset),\"Test data size:\", len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 및 Training Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=10, verbose=False, delta=0):\n",
    "        '''\n",
    "        patience (int): 얼마나 기다릴지\n",
    "        verbose (bool): True일 경우 각 epoch의 loss 출력\n",
    "        delta (float): 개선이 되었다고 인정되는 최소한의 loss\n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss # validation loss가 작을수록 좋다고 가정\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리드 서치 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy, os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Auto-scaled learning rate based on batch size\n",
    "lr = 1e-3 * BATCH_SIZE / 256  # 1e-3 is the default learning rate for AdamW in PyTorch\n",
    "\n",
    "class GridSearchTrainer:\n",
    "    def __init__(self, models, criterions, train_loader, val_loader, n_epochs=50, patience=10, save_dir='./checkpoints', verbose=True, device=None):\n",
    "        \"\"\"\n",
    "        models: {\"model_name\": model()}\n",
    "        criterions: {\"loss_name\": loss_function}\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.criterions = criterions\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.patience = patience\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "        self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    def train_one_epoch(self, model, optimizer, criterion):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(self.train_loader, desc=\"Train\", leave=False)\n",
    "        for x, _ in pbar:\n",
    "            x = x.to(self.device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output, x)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def validate_one_epoch(self, model, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(self.val_loader, desc=\"Validation\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for x, _ in pbar:\n",
    "                x = x.to(self.device)\n",
    "                output = model(x)\n",
    "                loss = criterion(output, x)\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "        return total_loss / len(self.val_loader)\n",
    "\n",
    "    def run(self):\n",
    "        results = []\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            for loss_name, criterion in self.criterions.items():\n",
    "                print(f'▶ Training [{model_name}] with [{loss_name}]')\n",
    "\n",
    "                model = model.to(self.device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "                criterion = criterion.to(self.device)\n",
    "                best_val_loss = float('inf')\n",
    "                best_model = None\n",
    "                early_stop_counter = 0\n",
    "\n",
    "                # ✅ tqdm으로 epoch 진행률 표시\n",
    "                for epoch in trange(self.n_epochs, desc=f\"{model_name} | {loss_name}\"):\n",
    "                    train_loss = self.train_one_epoch(model, optimizer, criterion)\n",
    "                    val_loss = self.validate_one_epoch(model, criterion)\n",
    "\n",
    "                    if self.verbose:\n",
    "                        print(f'[Epoch {epoch+1}/{self.n_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_model = copy.deepcopy(model.state_dict())\n",
    "                        early_stop_counter = 0\n",
    "                        if self.verbose:\n",
    "                            print(f'>> Best Updated (Val Loss: {best_val_loss:.4f})')\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "\n",
    "                    if early_stop_counter >= self.patience:\n",
    "                        if self.verbose:\n",
    "                            print(f'>> Early Stopping at Epoch {epoch+1}')\n",
    "                        break\n",
    "\n",
    "                clean_loss_name = loss_name.replace(\"+\", \"and\")\n",
    "                save_path = f'{self.save_dir}/{model_name}_{clean_loss_name}.pth'\n",
    "                torch.save(best_model, save_path)\n",
    "                print(f'>> Saved Best [{model_name}] + [{loss_name}] -> {save_path}')\n",
    "\n",
    "                results.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"loss\": loss_name,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"save_path\": save_path\n",
    "                })\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 및 loss 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: dict_keys(['Autoencoder', 'Autoencoder2D', 'CAE', 'DenoisingAutoencoder', 'DiffusionUNet', 'GANDiscriminator', 'GANGenerator', 'GANomaly', 'PatchEmbed', 'RobustAutoencoder', 'SimpleDDPM', 'SkipConnectionAutoencoder', 'TransformerAnomalyDetector', 'VAE'])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import model\n",
    "\n",
    "def get_model_classes():\n",
    "    \"\"\"\n",
    "    model 폴더 내에서 nn.Module 기반 클래스만 자동으로 dict로 반환\n",
    "    \"\"\"\n",
    "    model_classes = {}\n",
    "    for k in dir(model):\n",
    "        obj = getattr(model, k)\n",
    "        if isinstance(obj, type) and issubclass(obj, nn.Module) and obj.__module__.startswith('model.'):\n",
    "            model_classes[k] = obj\n",
    "    return model_classes\n",
    "\n",
    "model_classes = {name: cls() for name, cls in get_model_classes().items()}\n",
    "print(\"Available models:\", model_classes.keys())\n",
    "\n",
    "# loss function\n",
    "from loss.losses import FlexibleLoss\n",
    "loss_functions = {\n",
    "    \"MSE\": FlexibleLoss(mode=\"mse\"),\n",
    "    \"MSE+SSIM\": FlexibleLoss(mode=\"mse+ssim\"),\n",
    "    \"MSE+SSIM+Perceptual\": FlexibleLoss(mode=\"mse+ssim+perceptual\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Training [Autoencoder] with [MSE]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Autoencoder | MSE:   4%|▍         | 2/50 [00:25<10:09, 12.69s/it]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "PATIENCE = 20\n",
    "trainer = GridSearchTrainer(\n",
    "    models=model_classes,\n",
    "    criterions=loss_functions,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    save_dir='./checkpoints',\n",
    "    verbose=False,\n",
    "    device=DEVICE\n",
    ")\n",
    "results = trainer.run()\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, checkpoint_dir, val_loader, test_loader, device=None, percentile=0.95, save_dir='./eval_results'):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.percentile = percentile\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def load_model(self, model_name):\n",
    "        model_class = get_model_classes()[model_name]\n",
    "        model = model_class().to(self.device)\n",
    "        return model\n",
    "\n",
    "    def get_scores(self, loader, model):\n",
    "        model.eval()\n",
    "        scores, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(loader, desc=\"Scoring\"):\n",
    "                x = x.to(self.device)\n",
    "                output = model(x)\n",
    "                if isinstance(output, tuple):\n",
    "                    output = output[1]\n",
    "                error = F.mse_loss(output, x, reduction='none')\n",
    "                score = error.view(error.size(0), -1).mean(dim=1)\n",
    "                scores.append(score.cpu())\n",
    "                labels.append(y.cpu())\n",
    "        return torch.cat(scores).numpy(), torch.cat(labels).numpy()\n",
    "\n",
    "    def run(self):\n",
    "        checkpoint_files = [f for f in os.listdir(self.checkpoint_dir) if f.endswith(\".pth\")]\n",
    "        results = []\n",
    "\n",
    "        for ckpt in checkpoint_files:\n",
    "            print(f\"\\n▶ Evaluating {ckpt}\")\n",
    "\n",
    "            model_name, loss_name = ckpt[:-4].split(\"_\", 1)\n",
    "            model = self.load_model(model_name)\n",
    "            model.load_state_dict(torch.load(os.path.join(self.checkpoint_dir, ckpt)))\n",
    "\n",
    "            # Validation으로 threshold 계산\n",
    "            val_scores, _ = self.get_scores(self.val_loader, model)\n",
    "            threshold = np.percentile(val_scores, self.percentile * 100)\n",
    "            print(f\" >> Threshold (@{self.percentile*100:.0f}%) = {threshold:.4f}\")\n",
    "\n",
    "            # Test 평가\n",
    "            test_scores, test_labels = self.get_scores(self.test_loader, model)\n",
    "            test_labels = (test_labels != 2).astype(int)  # 2 = normal class\n",
    "\n",
    "            preds = (test_scores > threshold).astype(int)\n",
    "\n",
    "            auc_score = roc_auc_score(test_labels, test_scores)\n",
    "            precision, recall, _ = precision_recall_curve(test_labels, test_scores)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            f1 = f1_score(test_labels, preds)\n",
    "\n",
    "            print(f\"ROC-AUC: {auc_score:.4f} | PR-AUC: {pr_auc:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "            results.append({\n",
    "                \"checkpoint\": ckpt,\n",
    "                \"threshold\": threshold,\n",
    "                \"roc_auc\": auc_score,\n",
    "                \"pr_auc\": pr_auc,\n",
    "                \"f1\": f1\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=DEVICE,\n",
    "    percentile=0.95,\n",
    "    save_dir='./eval_results'\n",
    ")\n",
    "eval_results = evaluator.run()\n",
    "eval_results.to_csv('./eval_results/eval_results.csv', index=False)\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
